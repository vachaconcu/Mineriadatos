{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaseTrabajo.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1AgLV3rCmV9k1bvpQk-8XuUP-dg4fXWaa",
      "authorship_tag": "ABX9TyNgJRzCxDCtO9znW9CxRyZ9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWld2r6LVcwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48352cdf-0458-4196-8f90-404546433e62"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gGrJDVCTLi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51dc3b1f-c908-47e2-cb0b-feb55179d793"
      },
      "source": [
        "from numpy import load\n",
        "dict_data = load('/content/drive/My Drive/Mineria/datos.npz')\n",
        "datos = dict_data['arr_0']\n",
        "datos=datos.reshape((23705,200,200,3))\n",
        "datos.shape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23705, 200, 200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKJKJuEpepTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(datos, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqzip73tnv-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data & model configuration\n",
        "img_width, img_height = X_train.shape[1], X_train.shape[2]\n",
        "batch_size = 64 #128\n",
        "no_epochs = 100 #100\n",
        "validation_split = 0.2 #20% validacion\n",
        "verbosity = 1 # Se muestran los resultados\n",
        "latent_dim = 2 #2 dimensiones de separacion del tensor?\n",
        "num_channels = 3 #planos de color\n",
        "\n",
        "# Reshape data\n",
        "input_train = X_train.reshape(X_train.shape[0], img_height, img_width, num_channels)\n",
        "input_test = X_test.reshape(X_test.shape[0], img_height, img_width, num_channels)\n",
        "input_shape = (img_height, img_width, num_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkGoBmyDmAnF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f7733384-40c0-47cf-e8d8-28f271c08bf2"
      },
      "source": [
        "input_train = input_train.astype('float32')\n",
        "input_test = input_test.astype('float32')\n",
        "print(\"input_train.shape=\",input_train.shape)\n",
        "print('input_test.shape',input_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_train.shape= (17778, 200, 200, 3)\n",
            "input_test.shape (5927, 200, 200, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZm1JdQYsJJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # =================\n",
        "# # Encoder\n",
        "# # =================\n",
        "\n",
        "# Definition\n",
        "i       = Input(shape=input_shape, name='encoder_input')\n",
        "cx      = Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(i) #facilita la comprensión de datos\n",
        "cx      = BatchNormalization()(cx) #Se asegura media 0 y varianza sigma\n",
        "cx      = Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(cx) #facilita la comprensión de datos\n",
        "cx      = BatchNormalization()(cx) #Se asegura media 0 y varianza sigma\n",
        "x       = Flatten()(cx) # one-dimensional shape\n",
        "x       = Dense(20, activation='relu')(x) #Autoencoder?\n",
        "x       = BatchNormalization()(x)\n",
        "mu      = Dense(latent_dim, name='latent_mu')(x) #parametros del encoder\n",
        "sigma   = Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "# Get Conv2D shape for Conv2DTranspose operation in decoder\n",
        "conv_shape = K.int_shape(cx) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_6UWaRyRkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "29605123-37d6-4a1f-d138-c2eb88ab7485"
      },
      "source": [
        "# Define sampling with reparameterization trick\n",
        "def sample_z(args):\n",
        "  mu, sigma = args\n",
        "  batch     = K.shape(mu)[0]\n",
        "  dim       = K.int_shape(mu)[1]\n",
        "  eps       = K.random_normal(shape=(batch, dim))\n",
        "  return mu + K.exp(sigma / 2) * eps\n",
        "\n",
        "# Use reparameterization trick to ....??\n",
        "z       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
        "\n",
        "# Instantiate encoder\n",
        "encoder = Model(i, [mu, sigma, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, 200, 200, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 100, 100, 8)  224         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 100, 100, 8)  32          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 50, 50, 16)   1168        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 50, 50, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 40000)        0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 20)           800020      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 20)           80          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "latent_mu (Dense)               (None, 2)            42          batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "latent_sigma (Dense)            (None, 2)            42          batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 2)            0           latent_mu[0][0]                  \n",
            "                                                                 latent_sigma[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 801,672\n",
            "Trainable params: 801,584\n",
            "Non-trainable params: 88\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3rTLnjxyx8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0d82cf0c-0995-4df4-dfbe-2cc55b4b842e"
      },
      "source": [
        "# =================\n",
        "# Decoder\n",
        "# =================\n",
        "# Definition\n",
        "d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
        "x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\n",
        "x     = BatchNormalization()(x)\n",
        "x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "cx    = Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "cx    = BatchNormalization()(cx)\n",
        "cx    = Conv2DTranspose(filters=8, kernel_size=3, strides=2, padding='same',  activation='relu')(cx)\n",
        "cx    = BatchNormalization()(cx)\n",
        "o     = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(cx)\n",
        "\n",
        "# Instantiate decoder\n",
        "decoder = Model(d_i, o, name='decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 40000)             120000    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 40000)             160000    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 50, 50, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 100, 100, 16)      2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 100, 100, 16)      64        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 200, 200, 8)       1160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 200, 200, 8)       32        \n",
            "_________________________________________________________________\n",
            "decoder_output (Conv2DTransp (None, 200, 200, 3)       219       \n",
            "=================================================================\n",
            "Total params: 283,795\n",
            "Trainable params: 203,747\n",
            "Non-trainable params: 80,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HnyGb3TzFJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "73907a65-0d63-4c1f-86b1-68d32dd2914f"
      },
      "source": [
        "# =================\n",
        "# VAE as a whole\n",
        "# =================\n",
        "\n",
        "vae_outputs = decoder(encoder(i)[2])\n",
        "vae         = Model(i, vae_outputs, name='vae')\n",
        "vae.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 200, 200, 3)       0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 2), (None, 2), (N 801672    \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 200, 200, 3)       283795    \n",
            "=================================================================\n",
            "Total params: 1,085,467\n",
            "Trainable params: 1,005,331\n",
            "Non-trainable params: 80,136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LszyJ0_zL3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss\n",
        "def kl_reconstruction_loss(true, pred):\n",
        "  # Reconstruction loss\n",
        "  reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height\n",
        "  # KL divergence loss\n",
        "  kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
        "  kl_loss = K.sum(kl_loss, axis=-1)\n",
        "  kl_loss *= -0.5\n",
        "  # Total loss = 50% rec + 50% KL divergence loss\n",
        "  return K.mean(reconstruction_loss + kl_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDkYVKHkzPL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eba4c5fb-3fdf-44c7-b54d-bdfa4f6e0f7c"
      },
      "source": [
        "# Compile VAE\n",
        "vae.compile(optimizer='adam', loss=kl_reconstruction_loss)\n",
        "\n",
        "# Train autoencoder\n",
        "vae.fit(input_train, input_train, epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14222 samples, validate on 3556 samples\n",
            "Epoch 1/100\n",
            "14222/14222 [==============================] - 137s 10ms/step - loss: 26371.4018 - val_loss: 26203.4002\n",
            "Epoch 2/100\n",
            "14222/14222 [==============================] - 138s 10ms/step - loss: 24938.5667 - val_loss: 25256.9050\n",
            "Epoch 3/100\n",
            "14222/14222 [==============================] - 137s 10ms/step - loss: 24816.7521 - val_loss: 24787.1484\n",
            "Epoch 4/100\n",
            "14222/14222 [==============================] - 138s 10ms/step - loss: 24761.2235 - val_loss: 24742.1716\n",
            "Epoch 5/100\n",
            "14222/14222 [==============================] - 135s 9ms/step - loss: 24735.7424 - val_loss: 24717.2721\n",
            "Epoch 6/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24712.4341 - val_loss: 24704.0448\n",
            "Epoch 7/100\n",
            "14222/14222 [==============================] - 137s 10ms/step - loss: 24700.6642 - val_loss: 24685.1815\n",
            "Epoch 8/100\n",
            "14222/14222 [==============================] - 135s 9ms/step - loss: 24690.4934 - val_loss: 24672.9622\n",
            "Epoch 9/100\n",
            "14222/14222 [==============================] - 138s 10ms/step - loss: 24670.9156 - val_loss: 24673.6173\n",
            "Epoch 10/100\n",
            "14222/14222 [==============================] - 135s 9ms/step - loss: 24661.0764 - val_loss: 24643.1741\n",
            "Epoch 11/100\n",
            "14222/14222 [==============================] - 136s 10ms/step - loss: 24656.8218 - val_loss: 24654.8621\n",
            "Epoch 12/100\n",
            "14222/14222 [==============================] - 134s 9ms/step - loss: 24650.8536 - val_loss: 24698.8724\n",
            "Epoch 13/100\n",
            "14222/14222 [==============================] - 137s 10ms/step - loss: 24643.9365 - val_loss: 24657.2639\n",
            "Epoch 14/100\n",
            "14222/14222 [==============================] - 135s 9ms/step - loss: 24631.9945 - val_loss: 24647.7805\n",
            "Epoch 15/100\n",
            "14222/14222 [==============================] - 136s 10ms/step - loss: 24623.6437 - val_loss: 24656.3072\n",
            "Epoch 16/100\n",
            "14222/14222 [==============================] - 135s 10ms/step - loss: 24637.4669 - val_loss: 24728.9900\n",
            "Epoch 17/100\n",
            "14222/14222 [==============================] - 137s 10ms/step - loss: 24619.9748 - val_loss: 24673.2328\n",
            "Epoch 18/100\n",
            "14222/14222 [==============================] - 138s 10ms/step - loss: 24617.0563 - val_loss: 24663.1013\n",
            "Epoch 19/100\n",
            "14222/14222 [==============================] - 135s 9ms/step - loss: 24612.0481 - val_loss: 24655.9212\n",
            "Epoch 20/100\n",
            "14222/14222 [==============================] - 136s 10ms/step - loss: 24613.4504 - val_loss: 24737.7038\n",
            "Epoch 21/100\n",
            "14222/14222 [==============================] - 138s 10ms/step - loss: 24620.8268 - val_loss: 24647.0490\n",
            "Epoch 22/100\n",
            "14222/14222 [==============================] - 140s 10ms/step - loss: 24615.1096 - val_loss: 24651.0897\n",
            "Epoch 23/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24606.2791 - val_loss: 24662.0626\n",
            "Epoch 24/100\n",
            "14222/14222 [==============================] - 150s 11ms/step - loss: 24600.5656 - val_loss: 24658.1442\n",
            "Epoch 25/100\n",
            "14222/14222 [==============================] - 142s 10ms/step - loss: 24591.3920 - val_loss: 24633.9773\n",
            "Epoch 26/100\n",
            "14222/14222 [==============================] - 148s 10ms/step - loss: 24584.8388 - val_loss: 24635.8691\n",
            "Epoch 27/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24576.2766 - val_loss: 24633.9284\n",
            "Epoch 28/100\n",
            "14222/14222 [==============================] - 136s 10ms/step - loss: 24589.1158 - val_loss: 24719.3221\n",
            "Epoch 29/100\n",
            "14222/14222 [==============================] - 136s 10ms/step - loss: 24576.0420 - val_loss: 24642.4878\n",
            "Epoch 30/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24572.0662 - val_loss: 24682.9888\n",
            "Epoch 31/100\n",
            "14222/14222 [==============================] - 135s 10ms/step - loss: 24566.5232 - val_loss: 24638.2282\n",
            "Epoch 32/100\n",
            "14222/14222 [==============================] - 140s 10ms/step - loss: 24557.4966 - val_loss: 24668.9319\n",
            "Epoch 33/100\n",
            "14222/14222 [==============================] - 140s 10ms/step - loss: 24561.5626 - val_loss: 24677.7508\n",
            "Epoch 34/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24557.5258 - val_loss: 24711.3000\n",
            "Epoch 35/100\n",
            "14222/14222 [==============================] - 142s 10ms/step - loss: 24557.4848 - val_loss: 24737.2059\n",
            "Epoch 36/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24555.1630 - val_loss: 24700.7284\n",
            "Epoch 37/100\n",
            "14222/14222 [==============================] - 136s 10ms/step - loss: 24551.0264 - val_loss: 24639.9991\n",
            "Epoch 38/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24543.6037 - val_loss: 24657.5051\n",
            "Epoch 39/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24549.6746 - val_loss: 24701.0637\n",
            "Epoch 40/100\n",
            "14222/14222 [==============================] - 140s 10ms/step - loss: 24538.3202 - val_loss: 24675.1131\n",
            "Epoch 41/100\n",
            "14222/14222 [==============================] - 141s 10ms/step - loss: 24531.7955 - val_loss: 24673.1478\n",
            "Epoch 42/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24532.7974 - val_loss: 24685.1952\n",
            "Epoch 43/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24534.3507 - val_loss: 24660.8803\n",
            "Epoch 44/100\n",
            "14222/14222 [==============================] - 139s 10ms/step - loss: 24535.6784 - val_loss: 24750.3448\n",
            "Epoch 45/100\n",
            "14222/14222 [==============================] - 141s 10ms/step - loss: 24534.2011 - val_loss: 24677.7885\n",
            "Epoch 46/100\n",
            "14222/14222 [==============================] - 140s 10ms/step - loss: 24521.2751 - val_loss: 24673.6869\n",
            "Epoch 47/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24516.9177 - val_loss: 24693.3782\n",
            "Epoch 48/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24512.4930 - val_loss: 24763.1299\n",
            "Epoch 49/100\n",
            "14222/14222 [==============================] - 148s 10ms/step - loss: 24514.0178 - val_loss: 24675.0188\n",
            "Epoch 50/100\n",
            "14222/14222 [==============================] - 150s 11ms/step - loss: 24521.4375 - val_loss: 24689.5853\n",
            "Epoch 51/100\n",
            "14222/14222 [==============================] - 149s 10ms/step - loss: 24509.5182 - val_loss: 24730.7569\n",
            "Epoch 52/100\n",
            "14222/14222 [==============================] - 151s 11ms/step - loss: 24509.6178 - val_loss: 24708.9593\n",
            "Epoch 53/100\n",
            "14222/14222 [==============================] - 147s 10ms/step - loss: 24522.0479 - val_loss: 24730.3343\n",
            "Epoch 54/100\n",
            "14222/14222 [==============================] - 142s 10ms/step - loss: 24516.4918 - val_loss: 24685.3237\n",
            "Epoch 55/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24506.1792 - val_loss: 24685.3277\n",
            "Epoch 56/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24513.8685 - val_loss: 24710.8741\n",
            "Epoch 57/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24503.2514 - val_loss: 24695.6772\n",
            "Epoch 58/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24519.6809 - val_loss: 24710.0220\n",
            "Epoch 59/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24514.9333 - val_loss: 24699.4995\n",
            "Epoch 60/100\n",
            "14222/14222 [==============================] - 147s 10ms/step - loss: 24516.0386 - val_loss: 24701.1791\n",
            "Epoch 61/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24506.8121 - val_loss: 24707.6873\n",
            "Epoch 62/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24499.2019 - val_loss: 24689.7280\n",
            "Epoch 63/100\n",
            "14222/14222 [==============================] - 142s 10ms/step - loss: 24505.4013 - val_loss: 24710.4917\n",
            "Epoch 64/100\n",
            "14222/14222 [==============================] - 146s 10ms/step - loss: 24499.1891 - val_loss: 24723.0972\n",
            "Epoch 65/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24502.6347 - val_loss: 24717.0678\n",
            "Epoch 66/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24502.9604 - val_loss: 24708.5760\n",
            "Epoch 67/100\n",
            "14222/14222 [==============================] - 146s 10ms/step - loss: 24496.5107 - val_loss: 24748.8008\n",
            "Epoch 68/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24491.6983 - val_loss: 24737.6986\n",
            "Epoch 69/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24512.3169 - val_loss: 24720.1451\n",
            "Epoch 70/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24490.0325 - val_loss: 24746.3995\n",
            "Epoch 71/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24486.2700 - val_loss: 24741.5087\n",
            "Epoch 72/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24490.8535 - val_loss: 24742.2466\n",
            "Epoch 73/100\n",
            "14222/14222 [==============================] - 148s 10ms/step - loss: 24486.7374 - val_loss: 24746.7058\n",
            "Epoch 74/100\n",
            "14222/14222 [==============================] - 153s 11ms/step - loss: 24482.9011 - val_loss: 24740.9172\n",
            "Epoch 75/100\n",
            "14222/14222 [==============================] - 148s 10ms/step - loss: 24480.6325 - val_loss: 24790.7556\n",
            "Epoch 76/100\n",
            "14222/14222 [==============================] - 156s 11ms/step - loss: 24480.5955 - val_loss: 24768.7988\n",
            "Epoch 77/100\n",
            "14222/14222 [==============================] - 148s 10ms/step - loss: 24475.0421 - val_loss: 24796.0643\n",
            "Epoch 78/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24485.9648 - val_loss: 24817.4666\n",
            "Epoch 79/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24476.6599 - val_loss: 24781.4821\n",
            "Epoch 80/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24478.3801 - val_loss: 24753.1169\n",
            "Epoch 81/100\n",
            "14222/14222 [==============================] - 146s 10ms/step - loss: 24475.4055 - val_loss: 24772.0166\n",
            "Epoch 82/100\n",
            "14222/14222 [==============================] - 146s 10ms/step - loss: 24485.8152 - val_loss: 24795.4079\n",
            "Epoch 83/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24474.9612 - val_loss: 24782.6033\n",
            "Epoch 84/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24460.4700 - val_loss: 24798.1592\n",
            "Epoch 85/100\n",
            "14222/14222 [==============================] - 143s 10ms/step - loss: 24469.0713 - val_loss: 24807.8864\n",
            "Epoch 86/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24467.1595 - val_loss: 24771.5944\n",
            "Epoch 87/100\n",
            "14222/14222 [==============================] - 145s 10ms/step - loss: 24470.0897 - val_loss: 24774.3245\n",
            "Epoch 88/100\n",
            "14222/14222 [==============================] - 144s 10ms/step - loss: 24467.1965 - val_loss: 24782.4909\n",
            "Epoch 89/100\n",
            "14222/14222 [==============================] - 146s 10ms/step - loss: 24469.0277 - val_loss: 24810.4009\n",
            "Epoch 90/100\n",
            "14222/14222 [==============================] - 148s 10ms/step - loss: 24475.9328 - val_loss: 24792.4008\n",
            "Epoch 91/100\n",
            "10048/14222 [====================>.........] - ETA: 38s - loss: 24480.4561"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM5uZU_U2EUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =================\n",
        "# Results visualization\n",
        "# Credits for original visualization code: https://keras.io/examples/variational_autoencoder_deconv/\n",
        "# (François Chollet).\n",
        "# Adapted to accomodate this VAE.\n",
        "# =================\n",
        "def viz_latent_space(encoder, data):\n",
        "  input_data, target_data = data\n",
        "  mu, _, _ = encoder.predict(input_data)\n",
        "  plt.figure(figsize=(8, 10))\n",
        "  plt.scatter(mu[:, 0], mu[:, 1], c=target_data)\n",
        "  plt.xlabel('z - dim 1')\n",
        "  plt.ylabel('z - dim 2')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "def viz_decoded(encoder, decoder, data):\n",
        "  num_samples = 15\n",
        "  figure = np.zeros((img_width * num_samples, img_height * num_samples, num_channels))\n",
        "  grid_x = np.linspace(-4, 4, num_samples)\n",
        "  grid_y = np.linspace(-4, 4, num_samples)[::-1]\n",
        "  for i, yi in enumerate(grid_y):\n",
        "      for j, xi in enumerate(grid_x):\n",
        "                z_sample = np.array([[xi, yi]])\n",
        "                x_decoded = decoder.predict(z_sample)\n",
        "                digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
        "                figure[i * img_width: (i + 1) * img_width,\n",
        "                  j * img_height: (j + 1) * img_height] = digit\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  start_range = img_width // 2\n",
        "  end_range = num_samples * img_width + start_range + 1\n",
        "  pixel_range = np.arange(start_range, end_range, img_width)\n",
        "  sample_range_x = np.round(grid_x, 1)\n",
        "  sample_range_y = np.round(grid_y, 1)\n",
        "  plt.xticks(pixel_range, sample_range_x)\n",
        "  plt.yticks(pixel_range, sample_range_y)\n",
        "  plt.xlabel('z - dim 1')\n",
        "  plt.ylabel('z - dim 2')\n",
        "  # matplotlib.pyplot.imshow() needs a 2D array, or a 3D array with the third dimension being of shape 3 or 4!\n",
        "  # So reshape if necessary\n",
        "  fig_shape = np.shape(figure)\n",
        "  if fig_shape[2] == 1:\n",
        "    figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
        "  # Show image\n",
        "  plt.imshow(figure)\n",
        "  plt.show()\n",
        "\n",
        "# Plot results\n",
        "data = (input_test, target_test)\n",
        "viz_latent_space(encoder, data)\n",
        "viz_decoded(encoder, decoder, data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}